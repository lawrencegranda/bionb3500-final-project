@misc{nadipalli2025,
  title         = {Layer-Wise Evolution of Representations in Fine-Tuned Transformers: Insights from Sparse AutoEncoders},
  author        = {Suneel Nadipalli},
  year          = {2025},
  eprint        = {2502.16722},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2502.16722}
}


@article{petukhova2025,
  title     = {Text clustering with large language model embeddings},
  volume    = {6},
  issn      = {2666-3074},
  url       = {http://dx.doi.org/10.1016/j.ijcce.2024.11.004},
  doi       = {10.1016/j.ijcce.2024.11.004},
  journal   = {International Journal of Cognitive Computing in Engineering},
  publisher = {Elsevier BV},
  author    = {Petukhova, Alina and Matos-Carvalho, Jo√£o P. and Fachada, Nuno},
  year      = {2025},
  month     = dec,
  pages     = {100-108}
}


@inproceedings{yenicelik2020,
  title     = {How does {BERT} capture semantics? A closer look at polysemous words},
  author    = {Yenicelik, David  and
               Schmidt, Florian  and
               Kilcher, Yannic},
  editor    = {Alishahi, Afra  and
               Belinkov, Yonatan  and
               Chrupa{\l}a, Grzegorz  and
               Hupkes, Dieuwke  and
               Pinter, Yuval  and
               Sajjad, Hassan},
  booktitle = {Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.blackboxnlp-1.15/},
  doi       = {10.18653/v1/2020.blackboxnlp-1.15},
  pages     = {156--162},
  abstract  = {The recent paradigm shift to contextual word embeddings has seen tremendous success across a wide range of down-stream tasks. However, little is known on how the emergent relation of context and semantics manifests geometrically. We investigate polysemous words as one particularly prominent instance of semantic organization. Our rigorous quantitative analysis of linear separability and cluster organization in embedding vectors produced by BERT shows that semantics do not surface as isolated clusters but form seamless structures, tightly coupled with sentiment and syntax.}
}
