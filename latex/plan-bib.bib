@article{nadipalli2025,
  title         = {Layer-Wise Evolution of Representations in Fine-Tuned Transformers: Insights from Sparse AutoEncoders},
  author        = {Suneel Nadipalli},
  year          = {2025},
  eprint        = {2502.16722},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2502.16722}
}


@article{petukhova2025,
  title     = {Text clustering with large language model embeddings},
  volume    = {6},
  issn      = {2666-3074},
  url       = {http://dx.doi.org/10.1016/j.ijcce.2024.11.004},
  doi       = {10.1016/j.ijcce.2024.11.004},
  journal   = {International Journal of Cognitive Computing in Engineering},
  publisher = {Elsevier BV},
  author    = {Petukhova, Alina and Matos-Carvalho, João P. and Fachada, Nuno},
  year      = {2025},
  month     = dec,
  pages     = {100-108}
}


@inproceedings{yenicelik2020,
  title     = {How does {BERT} capture semantics? {A} closer look at polysemous words},
  author    = {Yenicelik, David  and
               Schmidt, Florian  and
               Kilcher, Yannic},
  editor    = {Alishahi, Afra  and
               Belinkov, Yonatan  and
               Chrupa{\l}a, Grzegorz  and
               Hupkes, Dieuwke  and
               Pinter, Yuval  and
               Sajjad, Hassan},
  booktitle = {Proceedings of the Third {BlackboxNLP} Workshop on Analyzing and Interpreting Neural Networks for {NLP}},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.blackboxnlp-1.15/},
  doi       = {10.18653/v1/2020.blackboxnlp-1.15},
  pages     = {156--162},
  abstract  = {The recent paradigm shift to contextual word embeddings has seen tremendous success across a wide range of down-stream tasks. However, little is known on how the emergent relation of context and semantics manifests geometrically. We investigate polysemous words as one particularly prominent instance of semantic organization. Our rigorous quantitative analysis of linear separability and cluster organization in embedding vectors produced by BERT shows that semantics do not surface as isolated clusters but form seamless structures, tightly coupled with sentiment and syntax.}
}

@misc{gemini2025,
  author       = {Google},
  title        = {Gemini. {BIONB 3500}: {Contextual} Meaning in Transformers},
  year         = {2025},
  note         = {Generated by Google Gemini on 2025-10. Automated report obtained via Gemini's Research mode.},
  howpublished = {\url{https://gemini.google.com/share/8ef7ab3edcd1}}
}


@misc{deriving_contextualised_2021,
  author = {{ACL Anthology}},
  title  = {Deriving Contextualised Semantic Features from {BERT} (and Other Transformer Model) Embeddings},
  year   = {2021},
  url    = {https://aclanthology.org/2021.repl4nlp-1.26.pdf},
  note   = {Accessed 2025-10-16}
}


@misc{text_clustering_llm_2024v5,
  author = {Duan, Yuwei and Hu, Chao and He, Jiacheng},
  title  = {Text Clustering with Large Language Model Embeddings},
  year   = {2024},
  url    = {https://arxiv.org/html/2403.15112v5},
  note   = {Accessed 2025-10-16}
}


@book{openedition_2020,
  author    = {Paganelli, Federico},
  title     = {How Contextualized Word Embeddings Represent Word Senses},
  publisher = {Accademia University Press},
  year      = {2020},
  url       = {https://books.openedition.org/aaccademia/10872?lang=en},
  note      = {Accessed 2025-10-16}
}


@inproceedings{wiedemann2019bert,
  author    = {Wiedemann, Gregor and Remus, Steffen and Chawla, Avi and Biemann, Chris},
  title     = {Does {BERT} Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings},
  booktitle = {Proceedings of KONVENS},
  year      = {2019},
  url       = {https://www.inf.uni-hamburg.de/en/inst/ab/lt/publications/2019-wiedemannetal-konvens-bert.pdf},
  note      = {Accessed 2025-10-16}
}


@misc{contextual_sense_embeddings_2020,
  author = {Scarlini, Bianca and Pasini, Tommaso and Navigli, Roberto},
  title  = {With More Contexts Comes Better Performance: Contextualized Sense Embeddings for All-Round Word Sense Disambiguation},
  year   = {2020},
  url    = {https://aclanthology.org/2020.emnlp-main.285/},
  note   = {Accessed 2025-10-16}
}


@article{clustering_clusterability_2016,
  author  = {Hauer, Bradley and Kondrak, Grzegorz},
  title   = {Word Sense Clustering and Clusterability},
  journal = {Computational Linguistics},
  volume  = {42},
  number  = {2},
  pages   = {245--275},
  year    = {2016},
  doi     = {10.1162/COLI_a_00243},
  url     = {https://direct.mit.edu/coli/article/42/2/245/1532/Word-Sense-Clustering-and-Clusterability}
}


@misc{concept_induction_2024,
  author = {Kushnareva, Alena and Ustalov, Dmitry and Loukachevitch, Natalia},
  title  = {Concept Induction in Contextualized Representation Space},
  year   = {2024},
  url    = {https://arxiv.org/pdf/2406.20054},
  note   = {Accessed 2025-10-16}
}


@article{semi_supervised_wsd_2017,
  author  = {Yuan, Dayu and Perego, Andrea and Navigli, Roberto},
  title   = {Semi-supervised Learning with Induced Word Senses for State of the Art Word Sense Disambiguation},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {61},
  pages   = {445--503},
  year    = {2018},
  doi     = {10.1613/jair.5680},
  url     = {https://www.jair.org/index.php/jair/article/download/10999/26155/20499}
}


@misc{mono_poly_2021,
  author = {Amrami, Asaf and Goldberg, Yoav},
  title  = {Let's Play {Mono-Poly}: {BERT} Can Reveal Words' Polysemy Level and Partitionability into Senses},
  year   = {2021},
  url    = {https://www.researchgate.net/publication/351278953_Let%27s_Play_Mono-Poly_BERT_Can_Reveal_Words%27_Polysemy_Level_and_Partitionability_into_Senses},
  note   = {Accessed 2025-10-16}
}


@inproceedings{word_sense_induction_kd_2023,
  author    = {Barba, Edoardo and Sarti, Gabriele and Navigli, Roberto},
  title     = {Word Sense Induction with Knowledge Distillation from {BERT}},
  booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
  year      = {2023},
  url       = {https://arxiv.org/abs/2304.10642},
  note      = {Accessed 2025-10-16}
}


@inproceedings{revealing_dark_secrets_2019,
  author    = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  title     = {What Does {BERT} Look At? {An} Analysis of {BERT}'s Attention},
  booktitle = {Proceedings of the 2019 ACL Workshop BlackboxNLP},
  year      = {2019},
  url       = {https://aclanthology.org/D19-1445.pdf},
  note      = {Accessed 2025-10-16}
}


@article{fractal_semantic_convergence_2024,
  author  = {Molnar, András and Sch{"u}tze, Hinrich},
  title   = {Fractal Self-Similarity in Semantic Convergence: Gradient of Embedding Similarity across Transformer Layers},
  journal = {Information},
  volume  = {8},
  number  = {10},
  pages   = {552},
  year    = {2024},
  doi     = {10.3390/info8100552},
  url     = {https://www.mdpi.com/2504-3110/8/10/552}
}


@inproceedings{sensembert_2020,
  author    = {Scarlini, Bianca and Pasini, Tommaso and Navigli, Roberto},
  title     = {{SensEmBERT}: Context-Enhanced Sense Embeddings for Multilingual Word Sense Disambiguation},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year      = {2020},
  pages     = {8758--8766},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/6402/6258},
  note      = {Accessed 2025-10-16}
}


@article{distilbert_2019,
  author  = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  title   = {Distil{BERT}, a Distilled Version of {BERT}: Smaller, Faster, Cheaper and Lighter},
  journal = {arXiv preprint},
  year    = {2019},
  eprint  = {1910.01108},
  url     = {https://arxiv.org/pdf/1910.01108},
  note    = {Accessed 2025-10-16}
}


@misc{matchups_swiftorial_2025,
  author = {Swiftorial},
  title  = {Matchups: {BERT} vs {RoBERTa} vs {DistilBERT}},
  year   = {2025},
  url    = {https://www.swiftorial.com/matchups/nlp_platforms/bert-vs-roberta-vs-distilbert},
  note   = {Accessed 2025-10-16}
}


@misc{distilbert_zilliz_2024,
  author = {Zilliz},
  title  = {{DistilBERT}: {A} Smaller, Faster, and Distilled {BERT}},
  year   = {2024},
  url    = {https://zilliz.com/learn/distilbert-distilled-version-of-bert},
  note   = {Accessed 2025-10-16}
}


@inproceedings{arabic_wsi_2023,
  author    = {Altowayan, Ahmed and Alsowayan, Fahad and Alharbi, Abdullah},
  title     = {Sentence Transformers and {DistilBERT} for {Arabic} Word Sense Induction},
  booktitle = {Proceedings of the 12th International Conference on Pattern Recognition Applications and Methods},
  year      = {2023},
  pages     = {645--652},
  url       = {https://www.scitepress.org/Papers/2023/118917/118917.pdf},
  note      = {Accessed 2025-10-16}
}


@misc{distilbert_medium_2023,
  author = {Syahmisajid},
  title  = {Comparative Analysis of {DistilBERT} vs {BERT} for Fake News Detection: Performance and Testing Time},
  year   = {2023},
  url    = {https://medium.com/@syahmisajid12/comparative-analysis-of-distilbert-vs-bert-for-fake-news-detection-performance-and-testing-time-067dbf168511},
  note   = {Accessed 2025-10-16}
}


@misc{understanding_umap_2019,
  author = {Smilkov, Daniel and Carter, Shan},
  title  = {Understanding {UMAP}},
  year   = {2019},
  url    = {https://pair-code.github.io/understanding-umap/},
  note   = {Accessed 2025-10-16}
}


@misc{introduction_tsne_datacamp,
  author = {DataCamp},
  title  = {Introduction to t-{SNE}: Nonlinear Dimensionality Reduction and Data Visualization},
  year   = {2020},
  url    = {https://www.datacamp.com/tutorial/introduction-t-sne},
  note   = {Accessed 2025-10-16}
}
